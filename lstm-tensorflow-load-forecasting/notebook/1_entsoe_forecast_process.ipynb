{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Category Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the features: ['all', 'actual', 'entsoe', 'weather_t', 'weather_i', 'holiday', 'weekday', 'hour', 'month']\n",
    "model_cat_id = \"01\"\n",
    "feature = ['actual', 'entsoe']\n",
    "\n",
    "# LSTM layer configuration\n",
    "layer_conf = [ True, True, True]\n",
    "cells = [[ 5, 10, 20, 30, 50, 75, 100, 125, 150], [0, 10, 20, 50], [0, 10, 15, 20]]\n",
    "dropout = [0, 0.1, 0.2]\n",
    "batch_size = [8]\n",
    "timesteps = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select backend & Check if keras work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from decimal import *\n",
    "import pytz\n",
    "import time as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from numpy import newaxis\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as stattools\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (9, 5)\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from lstm_load import data, lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse: Root Mean Square Error - 模型预测值与实际值之间的差异\n",
    "\n",
    "mae: Mean Absolute Error - 衡量预测值与实际值的差异。\n",
    "\n",
    "mape: Mean Absolute Percentage Error - 衡量预测误差相对于实际值的百分比\n",
    "\n",
    "train_loss - 存储训练集上的损失值。损失函数是用于衡量模型预测误差的标准。\n",
    "\n",
    "valid_loss - 存储验证集上的损失值。用于评估模型在未见数据上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.path.dirname(''), '../data/fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('../data', 'fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = '../data/fulldataset.csv'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Denmark\\work\\project\\DS\\Forecast_power-trading\\lstm-tensorflow-load-forecasting\\data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "abspath = os.path.abspath('..\\data/fulldataset.csv')\n",
    "print(abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Denmark\\work\\project\\DS\\Forecast_power-trading\\lstm-tensorflow-load-forecasting\\data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.join(os.path.dirname(''), '../data/fulldataset.csv'))\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(os.path.abs(''), '../data/fulldataset.csv')\n",
    "abspath = os.path.abspath('../data/fulldataset.csv')\n",
    "loc_tz = pytz.timezone('Europe/Zurich')\n",
    "split_date = loc_tz.localize(dt.datetime(2017,2,1,0,0,0,0))\n",
    "validation_split = 0.2\n",
    "epochs = 30\n",
    "verbose = 0\n",
    "results = pd.DataFrame(columns=['module_name', 'config', 'dropout', 'train_loss', 'train_rmse', 'train_mae', 'train_mape', 'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape', 'test_rmse', 'test_mae', 'test_mape', 'epochs', 'batch_train', 'input_shape', 'total_time', 'time_step', 'splits'])\n",
    "early_stopping = True\n",
    "min_delta = 0.006\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTC: Coordinated Universal Time\n",
    "- BST: British Summer Time  -  UTC + 1\n",
    "- CEST: Central European Summer Time  -  UTC + 2\n",
    "夏天 - 英国比丹麦晚一小时，英国4pm，丹麦5pm\n",
    "\n",
    "------ Winter ------\n",
    "-   UTC + 0\n",
    "- CET: Central European Time  -  UTC + 1\n",
    "\n",
    "时间按时区转换\n",
    "这要用到datetime模块的astimezone方法来实现。如下所示，开始生成本地时间，然后在转成utc时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now: 2024-06-12 14:44:19.514711 \n",
      "utc: 2024-06-12 12:44:19.514711+00:00 \n",
      "cet: 2024-06-12 14:44:19.514711+02:00\n"
     ]
    }
   ],
   "source": [
    "dt.datetime.now(pytz.timezone('CET'))\n",
    "utc = pytz.timezone('UTC')\n",
    "cet = pytz.timezone('CET')\n",
    "now_time = dt.datetime.now()\n",
    "utc_time = utc.normalize(now_time.astimezone(tz=utc))\n",
    "cet_time = cet.normalize(now_time.astimezone(tz=cet))\n",
    "print('now:', now_time, '\\nutc:', utc_time, '\\ncet:', cet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(model_name=None, layer_conf=None, cells=None, dropout=None, batch_size=None, timesteps=None):\n",
    "    models = []\n",
    "    layer_conb = list(itertools.product(*cells))\n",
    "    configs = [layer_conb, dropout, batch_size, timesteps]\n",
    "    combinations = list(itertools.product(*configs))\n",
    "\n",
    "    for ix, comb in enumerate(combinations):\n",
    "        m_name = model_name\n",
    "        m_name += str(ix + 1)\n",
    "\n",
    "        layers = []\n",
    "        for idx, level in enumerate(comb[0]):\n",
    "            return_sequence = True\n",
    "            if all(size == 0 for size in comb[0][idx + 1:]) == True:\n",
    "                return_sequence = False\n",
    "            if (idx + 1) == len(comb[0]):\n",
    "                return_sequence = False\n",
    "            if level > 0:\n",
    "                layers.append({'type': 'lstm', 'cell': level, 'dropout': comb[1], 'statful': layer_conf[idx], 'ret_seq': return_sequence })\n",
    "                m_name += '_1-' + str(comb[1])\n",
    "        if comb[1] > 0:\n",
    "            m_name += '_d-' + str(comb[1])\n",
    "        model_config = {\n",
    "            'name': m_name,\n",
    "            'layers': layers,\n",
    "            'batch_size': comb[2],\n",
    "            'timesteps': comb[3]\n",
    "        }\n",
    "        models.append(model_config)\n",
    "\n",
    "        print('==================')\n",
    "        print(tabulate([\n",
    "            ['Number of model configs generated', len(combinations)]],\n",
    "            tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "| Number of model configs generated | 432 |\n"
     ]
    }
   ],
   "source": [
    "result_dir = '../results/notebook_' + model_cat_id + '/'\n",
    "plot_dir = '../plots/notebook_' + model_cat_id + '/'\n",
    "model_dir = '../models/notebook_' + model_cat_id + '/'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "output_table = result_dir + model_cat_id + '_results_' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "test_output_table = result_dir + model_cat_id + '_test_results' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "\n",
    "models = []\n",
    "models = generate_combinations(\n",
    "    model_name=model_cat_id + '_', layer_conf=layer_conf, cells=cells, dropout=dropout,\n",
    "    batch_size=batch_size,timesteps=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cell': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading - preprocess, standardize & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.load_dataset(path=abspath, modules=feature)\n",
    "df_scaled = df.copy()\n",
    "df_scaled = df_scaled.dropna()\n",
    "floats = [key for key in dict(df_scaled.dtypes) if dict(df_scaled.dtypes)[key] in ['float64']]\n",
    "scaler =  StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df_scaled[floats]) # noraml distribution\n",
    "df_scaled[floats] = scaled_columns\n",
    "df_train = df_scaled.loc[(df_scaled.index < split_date)].copy()\n",
    "df_test = df_scaled.loc[df_scaled.index >= split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train['actual'].copy()\n",
    "X_train = df_train.drop('actual', axis=1).copy()\n",
    "y_test = df_test['actual'].copy()\n",
    "X_test = df_test.drop('actual', axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           actual  entsoe\n",
      "2014-12-31 23:00:00+00:00  7597.0  7654.0\n",
      "2015-01-01 00:00:00+00:00  7632.0  7687.0\n",
      "2015-01-01 01:00:00+00:00  7640.0  7696.0\n",
      "2015-01-01 02:00:00+00:00  7391.0  7448.0\n",
      "2015-01-01 03:00:00+00:00  7333.0  7382.0\n",
      "...                           ...     ...\n",
      "2017-05-16 17:00:00+00:00  7010.0  7020.0\n",
      "2017-05-16 18:00:00+00:00  6859.0  6865.0\n",
      "2017-05-16 19:00:00+00:00  6807.0  6731.0\n",
      "2017-05-16 20:00:00+00:00  6637.0  6666.0\n",
      "2017-05-16 21:00:00+00:00  6265.0  6258.0\n",
      "\n",
      "[20759 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'entsoe']\n"
     ]
    }
   ],
   "source": [
    "print(floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73256479  0.49798625]\n",
      " [ 0.76276426  0.52941582]\n",
      " [ 0.769667    0.53798752]\n",
      " ...\n",
      " [ 0.05091953 -0.38108926]\n",
      " [-0.09576363 -0.44299598]\n",
      " [-0.41674089 -0.83157974]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(scaled_columns, type(scaled_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             actual    entsoe\n",
      "2014-12-31 23:00:00+00:00  0.732565  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.762764  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.769667  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.554819  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.504774  0.238930\n",
      "...                             ...       ...\n",
      "2017-05-16 17:00:00+00:00  0.226076 -0.105842\n",
      "2017-05-16 18:00:00+00:00  0.095787 -0.253466\n",
      "2017-05-16 19:00:00+00:00  0.050920 -0.381089\n",
      "2017-05-16 20:00:00+00:00 -0.095764 -0.442996\n",
      "2017-05-16 21:00:00+00:00 -0.416741 -0.831580\n",
      "\n",
      "[20759 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_scaled[floats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             actual    entsoe\n",
      "2014-12-31 23:00:00+00:00  0.732565  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.762764  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.769667  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.554819  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.504774  0.238930\n",
      "...                             ...       ...\n",
      "2017-01-31 18:00:00+00:00  1.841317  2.215184\n",
      "2017-01-31 19:00:00+00:00  1.485826  1.562782\n",
      "2017-01-31 20:00:00+00:00  1.035422  0.861807\n",
      "2017-01-31 21:00:00+00:00  0.653183  1.024670\n",
      "2017-01-31 22:00:00+00:00  0.506500  0.290361\n",
      "\n",
      "[18240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-31 23:00:00+00:00    0.732565\n",
      "2015-01-01 00:00:00+00:00    0.762764\n",
      "2015-01-01 01:00:00+00:00    0.769667\n",
      "2015-01-01 02:00:00+00:00    0.554819\n",
      "2015-01-01 03:00:00+00:00    0.504774\n",
      "                               ...   \n",
      "2017-01-31 18:00:00+00:00    1.841317\n",
      "2017-01-31 19:00:00+00:00    1.485826\n",
      "2017-01-31 20:00:00+00:00    1.035422\n",
      "2017-01-31 21:00:00+00:00    0.653183\n",
      "2017-01-31 22:00:00+00:00    0.506500\n",
      "Name: actual, Length: 18240, dtype: float64 <class 'pandas.core.series.Series'>                              entsoe\n",
      "2014-12-31 23:00:00+00:00  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.238930\n",
      "...                             ...\n",
      "2017-01-31 18:00:00+00:00  2.215184\n",
      "2017-01-31 19:00:00+00:00  1.562782\n",
      "2017-01-31 20:00:00+00:00  0.861807\n",
      "2017-01-31 21:00:00+00:00  1.024670\n",
      "2017-01-31 22:00:00+00:00  0.290361\n",
      "\n",
      "[18240 rows x 1 columns] <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(y_train, type(y_train), X_train, type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models on all configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = t.time()\n",
    "for idx, m in enumerate(models):\n",
    "    stopper = t.time()\n",
    "    print('======= Model {}/{} ========'.format(idx+1, len(models)))\n",
    "    print(tabulate([['Starting with model', m['name']], ['Starting time', dt.fromtimestamp(stopper)]], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "    try:\n",
    "        model = lstm.create_model(layers=m['layers'], sample_size=X_train.shape[0], batch_size=m['batch_size'], timesteps=m['timesteps'], features=X_train.shape[1])\n",
    "        history = lstm.train_model(model=model, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=m['batch_size'], timesteps=m['timesteps'], epochs=epochs,\n",
    "                                   rearrange=False, validation_split=validation_split, verbose=verbose,\n",
    "                                   early_stopping=early_stopping, min_delta=min_delta, patience=patience)\n",
    "        min_loss = np.min(history.history['val_loss'])\n",
    "        min_idx = np.argmin(history.history['val_loss'])\n",
    "        min_epoch = min_idx + 1\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('_________________________')\n",
    "            print(tabulate([['Minimum validation loss at epoch', min_epoch, 'Time: {}'.format(t.time()-stopper)],\n",
    "                            ['Training loss & MAE', history.history['loss'][min_idx], history['mean_absolute_error'][min_idx] ],\n",
    "                            ['Validation loss & mae', history.history['val_loss'][min_idx], history.history['val']]\n",
    "                            ]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except BaseException as e:\n",
    "        print('======= ERROR {}/{} ======='.format(idx+1, len(models)))\n",
    "        print(tabulate([]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
