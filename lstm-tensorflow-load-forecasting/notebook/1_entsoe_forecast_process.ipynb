{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Category Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the features: ['all', 'actual', 'entsoe', 'weather_t', 'weather_i', 'holiday', 'weekday', 'hour', 'month']\n",
    "model_cat_id = \"01\"\n",
    "feature = ['actual', 'entsoe']\n",
    "\n",
    "# LSTM layer configuration\n",
    "layer_conf = [ True, True, True]\n",
    "cells = [[ 5, 10, 20, 30, 50, 75, 100, 125, 150], [0, 10, 20, 50], [0, 10, 15, 20]]\n",
    "dropout = [0, 0.1, 0.2]\n",
    "batch_size = [8]\n",
    "timesteps = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select backend & Check if keras work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from decimal import *\n",
    "import pytz\n",
    "import time as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from numpy import newaxis\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as stattools\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (9, 5)\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from lstm_load import data, lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse: Root Mean Square Error - 模型预测值与实际值之间的差异\n",
    "\n",
    "mae: Mean Absolute Error - 衡量预测值与实际值的差异。\n",
    "\n",
    "mape: Mean Absolute Percentage Error - 衡量预测误差相对于实际值的百分比\n",
    "\n",
    "train_loss - 存储训练集上的损失值。损失函数是用于衡量模型预测误差的标准。\n",
    "\n",
    "valid_loss - 存储验证集上的损失值。用于评估模型在未见数据上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.dirname(''), '../data/fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('../data', 'fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/fulldataset.csv'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspath = os.path.abspath('..\\data/fulldataset.csv')\n",
    "print(abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.path.join(os.path.dirname(''), '../data/fulldataset.csv'))\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(os.path.abs(''), '../data/fulldataset.csv')\n",
    "abspath = os.path.abspath('../data/fulldataset.csv')\n",
    "loc_tz = pytz.timezone('Europe/Zurich')\n",
    "split_date = loc_tz.localize(dt.datetime(2017,2,1,0,0,0,0))\n",
    "validation_split = 0.2\n",
    "epochs = 30\n",
    "verbose = 0\n",
    "results = pd.DataFrame(columns=['module_name', 'config', 'dropout', 'train_loss', 'train_rmse', 'train_mae', 'train_mape', 'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape', 'test_rmse', 'test_mae', 'test_mape', 'epochs', 'batch_train', 'input_shape', 'total_time', 'time_step', 'splits'])\n",
    "early_stopping = True\n",
    "min_delta = 0.006\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTC: Coordinated Universal Time\n",
    "- BST: British Summer Time  -  UTC + 1\n",
    "- CEST: Central European Summer Time  -  UTC + 2\n",
    "夏天 - 英国比丹麦晚一小时，英国4pm，丹麦5pm\n",
    "\n",
    "------ Winter ------\n",
    "-   UTC + 0\n",
    "- CET: Central European Time  -  UTC + 1\n",
    "\n",
    "时间按时区转换\n",
    "这要用到datetime模块的astimezone方法来实现。如下所示，开始生成本地时间，然后在转成utc时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.now(pytz.timezone('CET'))\n",
    "utc = pytz.timezone('UTC')\n",
    "cet = pytz.timezone('CET')\n",
    "now_time = dt.datetime.now()\n",
    "utc_time = utc.normalize(now_time.astimezone(tz=utc))\n",
    "cet_time = cet.normalize(now_time.astimezone(tz=cet))\n",
    "print('now:', now_time, '\\nutc:', utc_time, '\\ncet:', cet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(model_name=None, layer_conf=None, cells=None, dropout=None, batch_size=None, timesteps=None):\n",
    "    models = []\n",
    "    layer_conb = list(itertools.product(*cells))\n",
    "    configs = [layer_conb, dropout, batch_size, timesteps]\n",
    "    combinations = list(itertools.product(*configs))\n",
    "\n",
    "    for ix, comb in enumerate(combinations):\n",
    "        m_name = model_name\n",
    "        m_name += str(ix + 1)\n",
    "\n",
    "        layers = []\n",
    "        for idx, level in enumerate(comb[0]):\n",
    "            return_sequence = True\n",
    "            if all(size == 0 for size in comb[0][idx + 1:]) == True:\n",
    "                return_sequence = False\n",
    "            if (idx + 1) == len(comb[0]):\n",
    "                return_sequence = False\n",
    "            if level > 0:\n",
    "                layers.append({'type': 'lstm', 'cells': level, 'dropout': comb[1], 'stateful': layer_conf[idx], 'ret_seq': return_sequence })\n",
    "                m_name += '_1-' + str(comb[1])\n",
    "        if comb[1] > 0:\n",
    "            m_name += '_d-' + str(comb[1])\n",
    "        model_config = {\n",
    "            'name': m_name,\n",
    "            'layers': layers,\n",
    "            'batch_size': comb[2],\n",
    "            'timesteps': comb[3]\n",
    "        }\n",
    "        models.append(model_config)\n",
    "\n",
    "        print('==================')\n",
    "        print(tabulate([\n",
    "            ['Number of model configs generated', len(combinations)]],\n",
    "            tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "| Number of model configs generated | 432 |\n"
     ]
    }
   ],
   "source": [
    "result_dir = '../results/notebook_' + model_cat_id + '/'\n",
    "plot_dir = '../plots/notebook_' + model_cat_id + '/'\n",
    "model_dir = '../models/notebook_' + model_cat_id + '/'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "output_table = result_dir + model_cat_id + '_results_' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "test_output_table = result_dir + model_cat_id + '_test_results' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "\n",
    "models = []\n",
    "models = generate_combinations(\n",
    "    model_name=model_cat_id + '_', layer_conf=layer_conf, cells=cells, dropout=dropout,\n",
    "    batch_size=batch_size,timesteps=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cell': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading - preprocess, standardize & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.load_dataset(path=abspath, modules=feature)\n",
    "df_scaled = df.copy()\n",
    "df_scaled = df_scaled.dropna()\n",
    "floats = [key for key in dict(df_scaled.dtypes) if dict(df_scaled.dtypes)[key] in ['float64']]\n",
    "scaler =  StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df_scaled[floats]) # noraml distribution\n",
    "df_scaled[floats] = scaled_columns\n",
    "df_train = df_scaled.loc[(df_scaled.index < split_date)].copy()\n",
    "df_test = df_scaled.loc[df_scaled.index >= split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train['actual'].copy()\n",
    "X_train = df_train.drop('actual', axis=1).copy()\n",
    "y_test = df_test['actual'].copy()\n",
    "X_test = df_test.drop('actual', axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_columns, type(scaled_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled[floats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             actual    entsoe\n",
      "2014-12-31 23:00:00+00:00  0.732565  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.762764  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.769667  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.554819  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.504774  0.238930\n",
      "...                             ...       ...\n",
      "2017-01-31 18:00:00+00:00  1.841317  2.215184\n",
      "2017-01-31 19:00:00+00:00  1.485826  1.562782\n",
      "2017-01-31 20:00:00+00:00  1.035422  0.861807\n",
      "2017-01-31 21:00:00+00:00  0.653183  1.024670\n",
      "2017-01-31 22:00:00+00:00  0.506500  0.290361\n",
      "\n",
      "[18240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train, type(y_train), X_train, type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models on all configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Model 1/1 ========\n",
      "| Starting with model | 01_1_1-0                   |\n",
      "| Starting time       | 2024-06-17 16:41:40.130602 |\n",
      "Validated layers: [{'type': 'lstm', 'cells': 5, 'dropout': 0, 'stateful': True, 'ret_seq': False}]\n",
      "======= ERROR 1/1 =======\n",
      "| Model:  | 01_1_1-0                                                                                                                                          |\n",
      "| Config: | {'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cells': 5, 'dropout': 0, 'stateful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1} |\n",
      "Error: Unrecognized keyword arguments passed to LSTM: {'batch_input_shape': (8, 1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86176\\tf\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "start_time = t.time()\n",
    "\n",
    "def validate_layers(layers):\n",
    "    for layer in layers:\n",
    "        if 'cells' not in layer:\n",
    "            raise KeyError(f\"Missing 'cells' key in layer: {layer}\")\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for idx, m in enumerate(models):\n",
    "    stopper = t.time()\n",
    "    print('======= Model {}/{} ========'.format(idx+1, len(models)))\n",
    "    print(tabulate([['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]], \n",
    "                   tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "    try:\n",
    "        validate_layers(m['layers'])\n",
    "        print(\"Validated layers:\", m['layers'])  # Debugging output\n",
    "        \n",
    "        model = lstm.create_model(layers=m['layers'], sample_size=X_train.shape[0], batch_size=m['batch_size'], \n",
    "                                  timesteps=m['timesteps'], features=X_train.shape[1])\n",
    "        print(\"Model created successfully.\")  # Debugging output\n",
    "        \n",
    "        history = lstm.train_model(model=model, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=m['batch_size'], timesteps=m['timesteps'], epochs=epochs,\n",
    "                                   rearrange=False, validation_split=validation_split, verbose=verbose,\n",
    "                                   early_stopping=early_stopping, min_delta=min_delta, patience=patience)\n",
    "        print(\"Training completed.\")  # Debugging output\n",
    "        \n",
    "        min_loss = np.min(history.history['val_loss'])\n",
    "        min_idx = np.argmin(history.history['val_loss'])\n",
    "        min_epoch = min_idx + 1\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('_________________________')\n",
    "            print(tabulate([['Minimum validation loss at epoch', min_epoch, 'Time: {}'.format(t.time()-stopper)],\n",
    "                            ['Training loss & MAE', history.history['loss'][min_idx], history.history['mean_absolute_error'][min_idx] ],\n",
    "                            ['Validation loss & mae', history.history['val_loss'][min_idx], history.history['val_mean_absolute_error'][min_idx]],\n",
    "                            ], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "            print('_________________________')\n",
    "\n",
    "        result = pd.DataFrame([{'model_name': m['name'], 'config': m, 'train_loss': history.history['loss'][min_idx], 'train_rmse': 0,\n",
    "                   'train_mae': history.history['mean_absolute_error'][min_idx], 'train_mape': 0,\n",
    "                   'valid_loss': history.history['val_loss'][min_idx], 'valid_rmse': 0,\n",
    "                   'valid_mae': history.history['val_mean_absolute_error'][min_idx], 'valid_mape':0,\n",
    "                   'test_rmse':0, 'test_mae': 0, 'test_mape': 0, 'epochs': '{}/{}'.format(min_epoch, epochs), 'batch_train': m['batch_size'],\n",
    "                   'input_shape': (X_train.shape[0], timesteps, X_train.shape[1]), 'total_time': t.time()-stopper,\n",
    "                   'time_step': 0, 'splits': str(split_date), 'dropout': m['layers'][0]['dropout']                \n",
    "                   }])\n",
    "        \n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "        model.save(model_dir + m['name'] + '.h5')\n",
    "        results.to_csv(output_table, sep=';')\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Configuration error: {e}\")\n",
    "        continue    \n",
    "\n",
    "    except BaseException as e:\n",
    "        print('======= ERROR {}/{} ======='.format(idx+1, len(models)))\n",
    "        print(tabulate([['Model:', m['name']], ['Config:', m]], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        print('Error: {}'.format(e))\n",
    "        result = pd.DataFrame([{'model_name': m['name'], 'config':m, 'train_loss': str(e)}])\n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "        results.to_csv(output_table, sep=';')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 1/1 =========================\n",
      "| Starting with model | 01_1_1-0                   |\n",
      "| Starting time       | 2024-06-17 15:55:22.803229 |\n",
      "=============== ERROR 1/1 =============\n",
      "| Model:  | 01_1_1-0                                                                                                                                         |\n",
      "| Config: | {'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cells': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1} |\n",
      "Error: 'stateful'\n"
     ]
    }
   ],
   "source": [
    "start_time = t.time()\n",
    "for idx, m in enumerate(models):\n",
    "    stopper = t.time()\n",
    "    print('========================= Model {}/{} ========================='.format(idx+1, len(models)))\n",
    "    print(tabulate([['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]],\n",
    "                   tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "    try:\n",
    "        # Creating the Keras Model\n",
    "        model = lstm.create_model(layers=m['layers'], sample_size=X_train.shape[0], batch_size=m['batch_size'], \n",
    "                          timesteps=m['timesteps'], features=X_train.shape[1])\n",
    "        # Training...\n",
    "        history = lstm.train_model(model=model, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=m['batch_size'], timesteps=m['timesteps'], epochs=epochs, \n",
    "                                   rearrange=False, validation_split=validation_split, verbose=verbose, \n",
    "                                   early_stopping=early_stopping, min_delta=min_delta, patience=patience)\n",
    "\n",
    "        # Write results\n",
    "        min_loss = np.min(history.history['val_loss'])\n",
    "        min_idx = np.argmin(history.history['val_loss'])\n",
    "        min_epoch = min_idx + 1\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('______________________________________________________________________')\n",
    "            print(tabulate([['Minimum validation loss at epoch', min_epoch, 'Time: {}'.format(t.time()-stopper)],\n",
    "                        ['Training loss & MAE', history.history['loss'][min_idx], history.history['mean_absolute_error'][min_idx]  ], \n",
    "                        ['Validation loss & mae', history.history['val_loss'][min_idx], history.history['val_mean_absolute_error'][min_idx] ],\n",
    "                       ], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "            print('______________________________________________________________________')\n",
    "        \n",
    "        \n",
    "        result = [{'model_name': m['name'], 'config': m, 'train_loss': history.history['loss'][min_idx], 'train_rmse': 0,\n",
    "                   'train_mae': history.history['mean_absolute_error'][min_idx], 'train_mape': 0,\n",
    "                   'valid_loss': history.history['val_loss'][min_idx], 'valid_rmse': 0, \n",
    "                   'valid_mae': history.history['val_mean_absolute_error'][min_idx],'valid_mape': 0, \n",
    "                   'test_rmse': 0, 'test_mae': 0, 'test_mape': 0, 'epochs': '{}/{}'.format(min_epoch, epochs), 'batch_train':m['batch_size'],\n",
    "                   'input_shape':(X_train.shape[0], timesteps, X_train.shape[1]), 'total_time':t.time()-stopper, \n",
    "                   'time_step':0, 'splits':str(split_date), 'dropout': m['layers'][0]['dropout']\n",
    "                  }]\n",
    "        results = results.append(result, ignore_index=True)\n",
    "        \n",
    "        # Saving the model and weights\n",
    "        model.save(model_dir + m['name'] + '.h5')\n",
    "        \n",
    "        # Write results to csv\n",
    "        results.to_csv(output_table, sep=';')\n",
    "        \n",
    "        #if not os.path.isfile(output_table):\n",
    "            #results.to_csv(output_table, sep=';')\n",
    "        #else: # else it exists so append without writing the header\n",
    "        #    results.to_csv(output_table,mode = 'a',header=False, sep=';')\n",
    "        \n",
    "        K.clear_session()\n",
    "        import tensorflow as tf\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    # Shouldn't catch all errors, but for now...\n",
    "    except BaseException as e:\n",
    "        print('=============== ERROR {}/{} ============='.format(idx+1, len(models)))\n",
    "        print(tabulate([['Model:', m['name']], ['Config:', m]], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        print('Error: {}'.format(e))\n",
    "        result = pd.DataFrame([{'model_name': m['name'], 'config': m, 'train_loss': str(e)}])\n",
    "        results = pd.concat([results, result], ignore_index=True)\n",
    "        results.to_csv(output_table,sep=';')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cell': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result), type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cell': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1} <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(m, type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "\n",
    "stopper = t.time()\n",
    "# Process\n",
    "t.sleep(2)\n",
    "total_time = t.time() - stopper\n",
    "print(f\"Total time elapsed: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.datetime.fromtimestamp(t.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# print(history.history.keys())\n",
    "loss_history = history.history['loss']\n",
    "accuracy_history = history.history['accuracy']\n",
    "\n",
    "for epoch in range(len(loss_history)):\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss_history[epoch]}, Accuracy = {accuracy_history[epoch]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(10,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
