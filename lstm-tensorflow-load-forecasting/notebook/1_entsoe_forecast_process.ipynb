{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Category Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the features: ['all', 'actual', 'entsoe', 'weather_t', 'weather_i', 'holiday', 'weekday', 'hour', 'month']\n",
    "model_cat_id = \"01\"\n",
    "feature = ['actual', 'entsoe']\n",
    "\n",
    "# LSTM layer configuration\n",
    "layer_conf = [ True, True, True]\n",
    "cells = [[ 5, 10, 20, 30, 50, 75, 100, 125, 150], [0, 10, 20, 50], [0, 10, 15, 20]]\n",
    "dropout = [0, 0.1, 0.2]\n",
    "batch_size = [8]\n",
    "timesteps = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select backend & Check if keras work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from decimal import *\n",
    "import pytz\n",
    "import time as t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from numpy import newaxis\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as stattools\n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (9, 5)\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from lstm_load import data, lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse: Root Mean Square Error - 模型预测值与实际值之间的差异\n",
    "\n",
    "mae: Mean Absolute Error - 衡量预测值与实际值的差异。\n",
    "\n",
    "mape: Mean Absolute Percentage Error - 衡量预测误差相对于实际值的百分比\n",
    "\n",
    "train_loss - 存储训练集上的损失值。损失函数是用于衡量模型预测误差的标准。\n",
    "\n",
    "valid_loss - 存储验证集上的损失值。用于评估模型在未见数据上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.path.dirname(''), '../data/fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('../data', 'fulldataset.csv')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = '../data/fulldataset.csv'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Denmark\\work\\project\\DS\\Forecast_power-trading\\lstm-tensorflow-load-forecasting\\data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "abspath = os.path.abspath('..\\data/fulldataset.csv')\n",
    "print(abspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Denmark\\work\\project\\DS\\Forecast_power-trading\\lstm-tensorflow-load-forecasting\\data\\fulldataset.csv\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.join(os.path.dirname(''), '../data/fulldataset.csv'))\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(os.path.abs(''), '../data/fulldataset.csv')\n",
    "abspath = os.path.abspath('../data/fulldataset.csv')\n",
    "loc_tz = pytz.timezone('Europe/Zurich')\n",
    "split_date = loc_tz.localize(dt.datetime(2017,2,1,0,0,0,0))\n",
    "validation_split = 0.2\n",
    "epochs = 30\n",
    "verbose = 0\n",
    "results = pd.DataFrame(columns=['module_name', 'config', 'dropout', 'train_loss', 'train_rmse', 'train_mae', 'train_mape', 'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape', 'test_rmse', 'test_mae', 'test_mape', 'epochs', 'batch_train', 'input_shape', 'total_time', 'time_step', 'splits'])\n",
    "early_stopping = True\n",
    "min_delta = 0.006\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UTC: Coordinated Universal Time\n",
    "- BST: British Summer Time  -  UTC + 1\n",
    "- CEST: Central European Summer Time  -  UTC + 2\n",
    "夏天 - 英国比丹麦晚一小时，英国4pm，丹麦5pm\n",
    "\n",
    "------ Winter ------\n",
    "-   UTC + 0\n",
    "- CET: Central European Time  -  UTC + 1\n",
    "\n",
    "时间按时区转换\n",
    "这要用到datetime模块的astimezone方法来实现。如下所示，开始生成本地时间，然后在转成utc时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now: 2024-06-12 14:44:19.514711 \n",
      "utc: 2024-06-12 12:44:19.514711+00:00 \n",
      "cet: 2024-06-12 14:44:19.514711+02:00\n"
     ]
    }
   ],
   "source": [
    "dt.datetime.now(pytz.timezone('CET'))\n",
    "utc = pytz.timezone('UTC')\n",
    "cet = pytz.timezone('CET')\n",
    "now_time = dt.datetime.now()\n",
    "utc_time = utc.normalize(now_time.astimezone(tz=utc))\n",
    "cet_time = cet.normalize(now_time.astimezone(tz=cet))\n",
    "print('now:', now_time, '\\nutc:', utc_time, '\\ncet:', cet_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(model_name=None, layer_conf=None, cells=None, dropout=None, batch_size=None, timesteps=None):\n",
    "    models = []\n",
    "    layer_conb = list(itertools.product(*cells))\n",
    "    configs = [layer_conb, dropout, batch_size, timesteps]\n",
    "    combinations = list(itertools.product(*configs))\n",
    "\n",
    "    for ix, comb in enumerate(combinations):\n",
    "        m_name = model_name\n",
    "        m_name += str(ix + 1)\n",
    "\n",
    "        layers = []\n",
    "        for idx, level in enumerate(comb[0]):\n",
    "            return_sequence = True\n",
    "            if all(size == 0 for size in comb[0][idx + 1:]) == True:\n",
    "                return_sequence = False\n",
    "            if (idx + 1) == len(comb[0]):\n",
    "                return_sequence = False\n",
    "            if level > 0:\n",
    "                layers.append({'type': 'lstm', 'cell': level, 'dropout': comb[1], 'statful': layer_conf[idx], 'ret_seq': return_sequence })\n",
    "                m_name += '_1-' + str(comb[1])\n",
    "        if comb[1] > 0:\n",
    "            m_name += '_d-' + str(comb[1])\n",
    "        model_config = {\n",
    "            'name': m_name,\n",
    "            'layers': layers,\n",
    "            'batch_size': comb[2],\n",
    "            'timesteps': comb[3]\n",
    "        }\n",
    "        models.append(model_config)\n",
    "\n",
    "        print('==================')\n",
    "        print(tabulate([\n",
    "            ['Number of model configs generated', len(combinations)]],\n",
    "            tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all possible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "| Number of model configs generated | 432 |\n"
     ]
    }
   ],
   "source": [
    "result_dir = '../results/notebook_' + model_cat_id + '/'\n",
    "plot_dir = '../plots/notebook_' + model_cat_id + '/'\n",
    "model_dir = '../models/notebook_' + model_cat_id + '/'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "output_table = result_dir + model_cat_id + '_results_' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "test_output_table = result_dir + model_cat_id + '_test_results' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "\n",
    "models = []\n",
    "models = generate_combinations(\n",
    "    model_name=model_cat_id + '_', layer_conf=layer_conf, cells=cells, dropout=dropout,\n",
    "    batch_size=batch_size,timesteps=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '01_1_1-0', 'layers': [{'type': 'lstm', 'cell': 5, 'dropout': 0, 'statful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading - preprocess, standardize & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.load_dataset(path=abspath, modules=feature)\n",
    "df_scaled = df.copy()\n",
    "df_scaled = df_scaled.dropna()\n",
    "floats = [key for key in dict(df_scaled.dtypes) if dict(df_scaled.dtypes)[key] in ['float64']]\n",
    "scaler =  StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df_scaled[floats]) # noraml distribution\n",
    "df_scaled[floats] = scaled_columns\n",
    "df_train = df_scaled.loc[(df_scaled.index < split_date)].copy()\n",
    "df_test = df_scaled.loc[df_scaled.index >= split_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = df_train['actual'].copy()\n",
    "X_train = df_train.drop('actual', axis=1).copy()\n",
    "y_test = df_test['actual'].copy()\n",
    "X_test = df_test.drop('actual', axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           actual  entsoe\n",
      "2014-12-31 23:00:00+00:00  7597.0  7654.0\n",
      "2015-01-01 00:00:00+00:00  7632.0  7687.0\n",
      "2015-01-01 01:00:00+00:00  7640.0  7696.0\n",
      "2015-01-01 02:00:00+00:00  7391.0  7448.0\n",
      "2015-01-01 03:00:00+00:00  7333.0  7382.0\n",
      "...                           ...     ...\n",
      "2017-05-16 17:00:00+00:00  7010.0  7020.0\n",
      "2017-05-16 18:00:00+00:00  6859.0  6865.0\n",
      "2017-05-16 19:00:00+00:00  6807.0  6731.0\n",
      "2017-05-16 20:00:00+00:00  6637.0  6666.0\n",
      "2017-05-16 21:00:00+00:00  6265.0  6258.0\n",
      "\n",
      "[20759 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'entsoe']\n"
     ]
    }
   ],
   "source": [
    "print(floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73256479  0.49798625]\n",
      " [ 0.76276426  0.52941582]\n",
      " [ 0.769667    0.53798752]\n",
      " ...\n",
      " [ 0.05091953 -0.38108926]\n",
      " [-0.09576363 -0.44299598]\n",
      " [-0.41674089 -0.83157974]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(scaled_columns, type(scaled_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             actual    entsoe\n",
      "2014-12-31 23:00:00+00:00  0.732565  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.762764  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.769667  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.554819  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.504774  0.238930\n",
      "...                             ...       ...\n",
      "2017-05-16 17:00:00+00:00  0.226076 -0.105842\n",
      "2017-05-16 18:00:00+00:00  0.095787 -0.253466\n",
      "2017-05-16 19:00:00+00:00  0.050920 -0.381089\n",
      "2017-05-16 20:00:00+00:00 -0.095764 -0.442996\n",
      "2017-05-16 21:00:00+00:00 -0.416741 -0.831580\n",
      "\n",
      "[20759 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_scaled[floats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             actual    entsoe\n",
      "2014-12-31 23:00:00+00:00  0.732565  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.762764  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.769667  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.554819  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.504774  0.238930\n",
      "...                             ...       ...\n",
      "2017-01-31 18:00:00+00:00  1.841317  2.215184\n",
      "2017-01-31 19:00:00+00:00  1.485826  1.562782\n",
      "2017-01-31 20:00:00+00:00  1.035422  0.861807\n",
      "2017-01-31 21:00:00+00:00  0.653183  1.024670\n",
      "2017-01-31 22:00:00+00:00  0.506500  0.290361\n",
      "\n",
      "[18240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-12-31 23:00:00+00:00    0.732565\n",
      "2015-01-01 00:00:00+00:00    0.762764\n",
      "2015-01-01 01:00:00+00:00    0.769667\n",
      "2015-01-01 02:00:00+00:00    0.554819\n",
      "2015-01-01 03:00:00+00:00    0.504774\n",
      "                               ...   \n",
      "2017-01-31 18:00:00+00:00    1.841317\n",
      "2017-01-31 19:00:00+00:00    1.485826\n",
      "2017-01-31 20:00:00+00:00    1.035422\n",
      "2017-01-31 21:00:00+00:00    0.653183\n",
      "2017-01-31 22:00:00+00:00    0.506500\n",
      "Name: actual, Length: 18240, dtype: float64 <class 'pandas.core.series.Series'>                              entsoe\n",
      "2014-12-31 23:00:00+00:00  0.497986\n",
      "2015-01-01 00:00:00+00:00  0.529416\n",
      "2015-01-01 01:00:00+00:00  0.537988\n",
      "2015-01-01 02:00:00+00:00  0.301790\n",
      "2015-01-01 03:00:00+00:00  0.238930\n",
      "...                             ...\n",
      "2017-01-31 18:00:00+00:00  2.215184\n",
      "2017-01-31 19:00:00+00:00  1.562782\n",
      "2017-01-31 20:00:00+00:00  0.861807\n",
      "2017-01-31 21:00:00+00:00  1.024670\n",
      "2017-01-31 22:00:00+00:00  0.290361\n",
      "\n",
      "[18240 rows x 1 columns] <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(y_train, type(y_train), X_train, type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training models on all configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = t.time()\n",
    "for idx, m in enumerate(models):\n",
    "    stopper = t.time()\n",
    "    print('======= Model {}/{} ========'.format(idx+1, len(models)))\n",
    "    print(tabulate([['Starting with model', m['name']], ['Starting time', dt.datetime.fromtimestamp(stopper)]], \n",
    "                   tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "    try:\n",
    "        model = lstm.create_model(layers=m['layers'], sample_size=X_train.shape[0], batch_size=m['batch_size'], timesteps=m['timesteps'], features=X_train.shape[1])\n",
    "        history = lstm.train_model(model=model, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=m['batch_size'], timesteps=m['timesteps'], epochs=epochs,\n",
    "                                   rearrange=False, validation_split=validation_split, verbose=verbose,\n",
    "                                   early_stopping=early_stopping, min_delta=min_delta, patience=patience)\n",
    "        min_loss = np.min(history.history['val_loss'])\n",
    "        min_idx = np.argmin(history.history['val_loss'])\n",
    "        min_epoch = min_idx + 1\n",
    "\n",
    "        if verbose > 0:\n",
    "            print('_________________________')\n",
    "            print(tabulate([['Minimum validation loss at epoch', min_epoch, 'Time: {}'.format(t.time()-stopper)],\n",
    "                            ['Training loss & MAE', history.history['loss'][min_idx], history['mean_absolute_error'][min_idx] ],\n",
    "                            ['Validation loss & mae', history.history['val_loss'][min_idx], history.history['val_mean_absolute_error'][min_idx]],\n",
    "                            ], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "            print('_________________________')\n",
    "\n",
    "        result = [{'model_name': m['name'], 'config': m, 'train_loss': history.history['loss'][min_idx], 'train_rmse': 0,\n",
    "                   'train_mae': history.history['mean_absolute_error'][min_idx], 'train_mape': 0,\n",
    "                   'valid_loss': history.history['val_loss'][min_idx], 'valid_rmse': 0,\n",
    "                   'valid_mae': history.history['val_mean_absolute_error'][min_idx], 'valid_mape':0,\n",
    "                   'test_rmse':0, 'test_mae': 0, 'test_mape': 0, 'epochs': '{}/{}'.format(min_epoch, epochs), 'batch_train': m['batch_size'],\n",
    "                   'input_shape': (X_train.shape[0], timesteps, X_train.shape[1]), 'total_time': t.time()-stopper,\n",
    "                   'time_step': 0, 'splits': str(split_date), 'dropout': m['layers'][0]['dropout']                \n",
    "                   }]\n",
    "\n",
    "\n",
    "\n",
    "    except BaseException as e:\n",
    "        print('======= ERROR {}/{} ======='.format(idx+1, len(models)))\n",
    "        print(tabulate([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time elapsed: 2.000364065170288 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "\n",
    "stopper = t.time()\n",
    "# Process\n",
    "t.sleep(2)\n",
    "total_time = t.time() - stopper\n",
    "print(f\"Total time elapsed: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:10:36.369292\n"
     ]
    }
   ],
   "source": [
    "print(dt.datetime.fromtimestamp(t.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86176\\tf\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.5305 - loss: 0.6908 - val_accuracy: 0.5000 - val_loss: 0.7178\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5977 - loss: 0.6843 - val_accuracy: 0.5500 - val_loss: 0.7142\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5688 - loss: 0.6831 - val_accuracy: 0.5000 - val_loss: 0.7091\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6086 - loss: 0.6800 - val_accuracy: 0.5000 - val_loss: 0.7049\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5773 - loss: 0.6788 - val_accuracy: 0.5000 - val_loss: 0.7006\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5891 - loss: 0.6719 - val_accuracy: 0.5000 - val_loss: 0.6981\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5977 - loss: 0.6765 - val_accuracy: 0.5500 - val_loss: 0.6972\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6016 - loss: 0.6743 - val_accuracy: 0.5500 - val_loss: 0.6949\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6234 - loss: 0.6682 - val_accuracy: 0.5500 - val_loss: 0.6937\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6391 - loss: 0.6654 - val_accuracy: 0.5500 - val_loss: 0.6913\n",
      "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
      "Epoch 1: Loss = 0.691568911075592, Accuracy = 0.5375000238418579\n",
      "Epoch 2: Loss = 0.685035765171051, Accuracy = 0.5625\n",
      "Epoch 3: Loss = 0.6811479330062866, Accuracy = 0.574999988079071\n",
      "Epoch 4: Loss = 0.6775967478752136, Accuracy = 0.6000000238418579\n",
      "Epoch 5: Loss = 0.6752458810806274, Accuracy = 0.6000000238418579\n",
      "Epoch 6: Loss = 0.6724740862846375, Accuracy = 0.6000000238418579\n",
      "Epoch 7: Loss = 0.6692840456962585, Accuracy = 0.625\n",
      "Epoch 8: Loss = 0.6671782732009888, Accuracy = 0.625\n",
      "Epoch 9: Loss = 0.6645159125328064, Accuracy = 0.637499988079071\n",
      "Epoch 10: Loss = 0.6621647477149963, Accuracy = 0.637499988079071\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# print(history.history.keys())\n",
    "loss_history = history.history['loss']\n",
    "accuracy_history = history.history['accuracy']\n",
    "\n",
    "for epoch in range(len(loss_history)):\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss_history[epoch]}, Accuracy = {accuracy_history[epoch]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5547 - loss: 0.6808 - val_accuracy: 0.5500 - val_loss: 0.7071\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5445 - loss: 0.6781 - val_accuracy: 0.5500 - val_loss: 0.7093\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5508 - loss: 0.6750 - val_accuracy: 0.5500 - val_loss: 0.7113\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5586 - loss: 0.6703 - val_accuracy: 0.5500 - val_loss: 0.7135\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5508 - loss: 0.6689 - val_accuracy: 0.4500 - val_loss: 0.7160\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5664 - loss: 0.6671 - val_accuracy: 0.4000 - val_loss: 0.7189\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5430 - loss: 0.6667 - val_accuracy: 0.4000 - val_loss: 0.7216\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5883 - loss: 0.6575 - val_accuracy: 0.4000 - val_loss: 0.7245\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5789 - loss: 0.6598 - val_accuracy: 0.4000 - val_loss: 0.7274\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6008 - loss: 0.6610 - val_accuracy: 0.4000 - val_loss: 0.7299\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(10,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
